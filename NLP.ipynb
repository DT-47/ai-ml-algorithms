{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790c63d6",
      "metadata": {
        "id": "790c63d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1721a6fc",
      "metadata": {
        "id": "1721a6fc"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6a8e74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b6a8e74",
        "outputId": "cbb6de37-a582-485e-852a-196eadc0c377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world This is a sample text\n"
          ]
        }
      ],
      "source": [
        "text=\"Hello, world! This is a sample text.\"\n",
        "cleaned_text=re.sub(r'[^\\w\\s]','',text)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9421588b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9421588b",
        "outputId": "8c409b5c-08ca-4d96-af3c-458bc12683d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have  apples and  oranges.\n"
          ]
        }
      ],
      "source": [
        "text=\"I have 5 apples and 3 oranges.\"\n",
        "cleaned_text=re.sub(r'\\d+','',text)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4f0549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce4f0549",
        "outputId": "71e619de-1983-4f5c-acdc-f76b2579ba7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check out my website: \n"
          ]
        }
      ],
      "source": [
        "text=\"Check out my website: https://www.example.com\"\n",
        "cleaned_text=re.sub(r'https\\S+|www\\S+|\\S+.com\\S+','',text)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4fb2a66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4fb2a66",
        "outputId": "4162844d-0b23-4e06-c59d-219c744ba9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample sentence stopwords removed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        " import re\n",
        " import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "text=\"This is a sample sentence with stopwords that should be removed\"\n",
        "cleaned_text=' '.join(word for word in text.split() if word.lower() not in stopwords)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebaee5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ebaee5f",
        "outputId": "4e37727f-0623-414c-8aa6-b558c6b60e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945745873\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text='extract phone number: 945745873'\n",
        "cleaned_data=re.sub(r'[^0-9]','',text)\n",
        "print(cleaned_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f863d57b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f863d57b",
        "outputId": "0b8a945c-efb2-433e-cf37-0e75757fd2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phone Numbers:\n",
            "342-354-4350\n",
            "542-453-4530\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def extract_phone_numbers(text):\n",
        "    pattern=r'\\b\\d{3}-\\d{3}-\\d{4}\\b'\n",
        "    phone_numbers=re.findall(pattern,text)\n",
        "    print(\"Phone Numbers:\")\n",
        "    for num in phone_numbers:\n",
        "        print(num)\n",
        "text=\"Contact me at 342-354-4350 or 542-453-4530.\"\n",
        "extract_phone_numbers(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7da0f90",
      "metadata": {
        "id": "c7da0f90"
      },
      "source": [
        "## Tokenization of word and sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8de5e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "6f8de5e7",
        "outputId": "bac01a89-dba5-40fa-dddd-6a0cfad2a4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-32ba986e38a9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "sentence=\"\"\"3 I envision an India where every citizen, regardless of their background or circumstances, is empowered to dream, strive, and achieve their fullest potential.\n",
        "\"\"\"\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tokens=nltk.word_tokenize(sentence)\n",
        "pos_tags=nltk.pos_tag(tokens)\n",
        "for word, tag in pos_tags:\n",
        "    print(word,\"        \",tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbe7f0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "4fbe7f0b",
        "outputId": "9992e514-5cf4-428b-bf89-5ca16a8f09b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8351d0df70db>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m Thank you, and Jai Hind!\"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#Stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "paragraph=\"\"\"I have three vision for India.\n",
        "Ladies and gentlemen,\n",
        "Today, I stand before you to share my vision for India—a vision that is rooted in my unwavering belief in the potential and capabilities of our great nation. I envision an India where every citizen, regardless of their background or circumstances, is empowered to dream, strive, and achieve their fullest potential.\n",
        "In my vision of India, education plays a pivotal role. I believe that education is the key to unlocking the doors of progress and development. I envision a nation where every child has access to quality education, where schools become centers of excellence, nurturing creativity, critical thinking, and innovation. An India where knowledge is valued and lifelong learning is embraced.\n",
        "Moreover, my vision extends beyond education. I envision a technologically advanced India, where science and technology are harnessed to address the challenges facing our society. A nation where research and innovation flourish, where our young minds become leaders in cutting-edge fields, and where the benefits of technological advancements reach every corner of our country.\n",
        "In my vision of India, inclusivity and social justice are paramount. I dream of a nation where no individual is left behind, where equality is not just a word but a reality. I envision a society where the marginalized are uplifted, where gender equality is the norm, and where diversity is celebrated as a strength.\n",
        "Furthermore, my vision embraces sustainable development and environmental stewardship. I believe that India must become a global leader in preserving our planet for future generations. I envision a country that prioritizes renewable energy, sustainable practices, and environmental conservation. An India that takes pride in its rich natural heritage and works tirelessly to protect it.\n",
        "Lastly, my vision for India is one of global leadership and peace. I envision an India that engages with the world as a responsible and compassionate global citizen. A nation that fosters peace, harmony, and cooperation among nations, working towards a world free from conflicts and divisions.\n",
        "In conclusion, my vision for India is driven by the principles of education, technology, inclusivity, sustainability, and global leadership. It is a vision that calls upon each one of us to contribute our best, to believe in our potential, and to work tirelessly towards realizing the dream of a prosperous, progressive, and harmonious India.\n",
        "Thank you, and Jai Hind!\"\"\"\n",
        "\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "stemmer=PorterStemmer()\n",
        "#Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words= nltk.word_tokenize(sentences[i])\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)\n",
        "    print(sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe78228",
      "metadata": {
        "id": "ffe78228"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "paragraph=\"\"\"I have three vision for India.\n",
        "Ladies and gentlemen,\n",
        "Today, I stand before you to share my vision for India—a vision that is rooted in my unwavering belief in the potential and capabilities of our great nation. I envision an India where every citizen, regardless of their background or circumstances, is empowered to dream, strive, and achieve their fullest potential.\n",
        "In my vision of India, education plays a pivotal role. I believe that education is the key to unlocking the doors of progress and development. I envision a nation where every child has access to quality education, where schools become centers of excellence, nurturing creativity, critical thinking, and innovation. An India where knowledge is valued and lifelong learning is embraced.\n",
        "Moreover, my vision extends beyond education. I envision a technologically advanced India, where science and technology are harnessed to address the challenges facing our society. A nation where research and innovation flourish, where our young minds become leaders in cutting-edge fields, and where the benefits of technological advancements reach every corner of our country.\n",
        "In my vision of India, inclusivity and social justice are paramount. I dream of a nation where no individual is left behind, where equality is not just a word but a reality. I envision a society where the marginalized are uplifted, where gender equality is the norm, and where diversity is celebrated as a strength.\n",
        "Furthermore, my vision embraces sustainable development and environmental stewardship. I believe that India must become a global leader in preserving our planet for future generations. I envision a country that prioritizes renewable energy, sustainable practices, and environmental conservation. An India that takes pride in its rich natural heritage and works tirelessly to protect it.\n",
        "Lastly, my vision for India is one of global leadership and peace. I envision an India that engages with the world as a responsible and compassionate global citizen. A nation that fosters peace, harmony, and cooperation among nations, working towards a world free from conflicts and divisions.\n",
        "In conclusion, my vision for India is driven by the principles of education, technology, inclusivity, sustainability, and global leadership. It is a vision that calls upon each one of us to contribute our best, to believe in our potential, and to work tirelessly towards realizing the dream of a prosperous, progressive, and harmonious India.\n",
        "Thank you, and Jai Hind!\"\"\"\n",
        "\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "#Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words= nltk.word_tokenize(sentences[i])\n",
        "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)\n",
        "    print(sentences[i])\n",
        "    print(\"**********************\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ab253b",
      "metadata": {
        "id": "87ab253b"
      },
      "source": [
        "## bag of words model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bc435c",
      "metadata": {
        "id": "b6bc435c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "paragraph=\"\"\"Today, I stand before you to share my vision for India—a vision that is rooted in my unwavering belief in the potential and capabilities of our great nation.\n",
        "I envision an India where every citizen, regardless of their background or circumstances, is empowered to dream, strive, and achieve their fullest potential.\n",
        "\"\"\"\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "ps=PorterStemmer()\n",
        "wordnet=WordNetLemmatizer\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "corpus=[]\n",
        "for i in range(len(sentences)):\n",
        "    review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    review= [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58dacf6a",
      "metadata": {
        "id": "58dacf6a"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=10)\n",
        "x=cv.fit_transform(corpus).toarray()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3758a7",
      "metadata": {
        "id": "aa3758a7"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f340df03",
      "metadata": {
        "id": "f340df03"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "paragraph=\"\"\"Today, I stand before you to share my vision for India—a vision that is rooted in my unwavering belief in the potential and capabilities of our great nation.\n",
        "I envision an India where every citizen, regardless of their background or circumstances, is empowered to dream, strive, and achieve their fullest potential.\n",
        "\"\"\"\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "ps=PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "corpus=[]\n",
        "for i in range(len(sentences)):\n",
        "    review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    review= [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb069aa6",
      "metadata": {
        "id": "fb069aa6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer(max_features=10)\n",
        "x=cv.fit_transform(corpus).toarray()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2369229",
      "metadata": {
        "id": "b2369229"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "paragraph=\"\"\"Today, I stand before you to share my vision for India—a vision that is rooted in my unwavering belief in the potential and capabilities of our great nation.\n",
        "I envision an India where every citizen, regardless of their background or circumstances, is empowered to dream, strive, and achieve their fullest potential.\n",
        "\"\"\"\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "ps=PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()\n",
        "word2vec=Word2Vec()\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "corpus=[]\n",
        "print(sentences)\n",
        "print(\"*******************************\")\n",
        "for i in range(len(sentences)):\n",
        "    review=re.sub('\\d',' ',paragraph)\n",
        "    review=re.sub('\\s',' ',paragraph)\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    sentences[i]= [word for word in review if not word in set(stopwords.words('english'))]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)\n",
        "print(sentences)\n",
        "model=Word2Vec(sentences,min_count=1)\n",
        "print(model)\n",
        "print(\"########################################\")\n",
        "vector=model.wv['vision']\n",
        "print(vector)\n",
        "print(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\")\n",
        "for i in model.wv.most_similar('india'):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f156720",
      "metadata": {
        "id": "9f156720"
      },
      "source": [
        "## topic modelling with LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11bd014",
      "metadata": {
        "id": "a11bd014"
      },
      "outputs": [],
      "source": [
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "documents=[\n",
        "    \"The sky is blue\",\n",
        "    \"The sun is bright\",\n",
        "    \"The sun in the sky is bright\",\n",
        "    \"We can see the shining sun, the bright sun\",\n",
        "    \"The sky is cloudy\",\n",
        "    \"The rain is wet\",\n",
        "    \"Wet streets cause traffic jams\"\n",
        "]\n",
        "#preprocess the documents\n",
        "tokenized_docs=[doc.lower().split() for doc in documents]\n",
        "#create a dictionary representation of documents\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "#create bag of words\n",
        "corpus=[dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "#build LDA model\n",
        "lda_model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=2,random_state=42)\n",
        "print(\"Topics:\")\n",
        "for topic_id,topic in lda_model.show_topics(formatted=True):\n",
        "    print(f\"Topic #{topic_id+1}:{topic}\")\n",
        "\n",
        "coherence_model=CoherenceModel(model=lda_model,texts=tokenized_docs)\n",
        "coherence_score=coherence_model.get_coherence()\n",
        "print(f\"\\nCoherence Score:{coherence_score}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3368d78a",
      "metadata": {
        "id": "3368d78a"
      },
      "source": [
        "## coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950ed651",
      "metadata": {
        "id": "950ed651"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora, models\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a23c411",
      "metadata": {
        "id": "6a23c411"
      },
      "outputs": [],
      "source": [
        "news_groups_train=fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a16b7c0f",
      "metadata": {
        "id": "a16b7c0f"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame({'post':news_groups_train['data'], 'target':news_groups_train['target']})\n",
        "df['target_names']=df['target'].apply(lambda t:news_groups_train['target_names'][t])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51832a0",
      "metadata": {
        "id": "d51832a0"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    \" removes urls\"\n",
        "    url_pattern=re.compile(r'https?://\\S+|www.\\S+')\n",
        "    return url_pattern.sub(r'',text)\n",
        "def remove_html(text):\n",
        "    \"removes html tags\"\n",
        "    html_pattern = re.compile('')\n",
        "    return html_pattern.sub(r'',text)\n",
        "def remove_emails(text):\n",
        "    email_pattern=re.compile('\\S*@\\S*\\s?')\n",
        "    return email_pattern.sub(r'',text)\n",
        "def remove_new_line(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "def remove_non_alpha(text):\n",
        "    return re.sub(\"[^A-Za-z]+\",' ',str(text))\n",
        "def preprocess_text(text):\n",
        "    t=remove_urls(text)\n",
        "    t=remove_html(t)\n",
        "    t=remove_emails(t)\n",
        "    t=remove_new_line(t)\n",
        "    t=remove_non_alpha(t)\n",
        "    return t\n",
        "def lemmatize_words(text,lemmatizer):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "def remove_stopwords(text,stopwords):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stopwords])\n",
        "df['post_preprocessed']=df['post'].apply(preprocess_text).str.lower()\n",
        "print('lemming...')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "df['post_final']=df['post_preprocessed'].apply(lambda post:lemmatize_words(post,lemmatizer))\n",
        "print('remove stopwords...')\n",
        "nltk.download('stopwords')\n",
        "swords=set(stopwords.words('english'))\n",
        "df['post_final']=df['post_preprocessed'].apply(lambda post:remove_stopwords(post,swords))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4ed444",
      "metadata": {
        "id": "0b4ed444"
      },
      "outputs": [],
      "source": [
        "#bag of words\n",
        "posts=[x.split(' ') for x in df['post_final']]\n",
        "id2word=corpora.Dictionary(posts)\n",
        "corpus_tf=[id2word.doc2bow(text) for text in posts]\n",
        "print(corpus_tf[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "430ab933",
      "metadata": {
        "id": "430ab933"
      },
      "outputs": [],
      "source": [
        "#tfidf\n",
        "tfidf =models.TfidfModel(corpus_tf)\n",
        "corpus_tfidf=tfidf[corpus_tf]\n",
        "print(corpus_tfidf[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4639db51",
      "metadata": {
        "id": "4639db51"
      },
      "outputs": [],
      "source": [
        "model=LdaMulticore(corpus=corpus_tf,id2word=id2word,num_topics=20,\n",
        "                  alpha=0.1,eta=0.1,random_state=0)\n",
        "coherence=CoherenceModel(model=model,texts=posts,dictionary=id2word,coherence='u_mass')\n",
        "print(coherence.get_coherence())\n",
        "print(model.show_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5260d2c",
      "metadata": {
        "id": "a5260d2c"
      },
      "outputs": [],
      "source": [
        "pip install pyLDAvis --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7290e7",
      "metadata": {
        "id": "cb7290e7"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c699d16",
      "metadata": {
        "id": "5c699d16"
      },
      "outputs": [],
      "source": [
        "lda_display=gensimvis.prepare(model, corpus_tf,id2word, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f46bdfde",
      "metadata": {
        "id": "f46bdfde"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}